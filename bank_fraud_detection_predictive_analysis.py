# -*- coding: utf-8 -*-
"""Bank Fraud Detection - Predictive Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DjZJt4e8isxkhE3t2TIig8Z57Vub4KUT

# **1. *Library Setup***
"""

import numpy as np
import pandas as pd
import kagglehub
import matplotlib.pyplot as plt
import seaborn as sns
import folium
import warnings
warnings.filterwarnings('ignore')

from IPython.display import Image
from geopy.geocoders import Nominatim
from sklearn.cluster import KMeans,  DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage, fcluster
from datetime import datetime

"""# **2. *Data Loading***"""

"""
DOWNLOADING Bank Transaction Dataset
"""
! gdown 16pyvW9fYZBKFKCsUDKbzg4xQpmCwpCV5

"""
Overview of Dataset
"""
df = pd.read_csv('/content/bank_transactions_data_2.csv')

print("-" * 80)
print(df.info())
print("-" * 80)
df.head(5)

"""# **3. *Data Understanding***

### 3.1 *Exploratory Data Analysis* (EDA)

since our goals is to search for potential fraud transaction we won't erase the outliers
"""

df.describe()

unique_locations = df['Location'].unique()
location_coords = {}
geolocator = Nominatim(user_agent="location_mapper")
for location in unique_locations:
    try:
        loc = geolocator.geocode(location)
        if loc:
            location_coords[location] = (loc.latitude, loc.longitude)
        else:
            print(f"Coordinates not found for {location}")
    except Exception as e:
        print(f"Error fetching coordinates for {location} : {e}")

df['Coordinates'] = df['Location'].map(location_coords)

initial_coords = list(location_coords.values())[0] if location_coords else [0, 0]
mymap = folium.Map(location = initial_coords, zoom_start=5)

for _, row in df.iterrows():
    if row['Coordinates']:
        folium.Marker(
            location=row['Coordinates'],
            popup=f"TransactionID: {row['TransactionID']}<br>Amount: ${row['TransactionAmount']}",
            tooltip=row['Location']
        ).add_to(mymap)

mymap.save('/content/transaction_map.html')

! gdown 1DJmHtwDqT2A1KaimRPWtCI4CUrXf83qY

display(Image(filename='/content/bank_fraud_map.png'))

"""### 3.1.1 *Univariate Analysis*"""

# Create a figure with 2 rows and 3 columns
plt.figure(figsize=(20,12))
palette = sns.color_palette('mako', 6)

# 1. Transaction Type Distribution (Pie Chart)
debitcard_counts = df[df['TransactionType']=='Debit']['TransactionType'].count()
creditcard_counts = df[df['TransactionType']=='Credit']['TransactionType'].count()

plt.subplot(2,3,1)
index_values = [debitcard_counts, creditcard_counts]
index_labels = ['Debit', 'Credit']
plt.pie(index_values, labels=index_labels, autopct='%2.2f%%', colors=[palette[3], palette[4]])
plt.title('Transaction Type Distribution', fontsize=12)

# 2. Transaction Amount Distribution (Histogram)
plt.subplot(2,3,2)
sns.histplot(data=df, x="TransactionAmount", kde=True, hue='TransactionType', palette='mako')
plt.title("Transaction Amount Distribution", fontsize=12)
plt.xlabel("Transaction Amount")
plt.ylabel("Count")

# 3. Transaction Location Distribution (Bar Chart)
location_counts = df['Location'].value_counts()

plt.subplot(2,3,3)
plt.bar(location_counts.index, location_counts.values, color=palette[3], edgecolor='black')
plt.title('Transaction Location Distribution', fontsize=12)
plt.xlabel('Location')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=90)

# 4. Transfer Channel Distribution (Pie Chart)
branch_counts = df[df['Channel']=='Branch']['Channel'].count()
atm_counts = df[df['Channel']=='ATM']['Channel'].count()
online_counts = df[df['Channel']=='Online']['Channel'].count()

plt.subplot(2,3,4)
channel_values = [branch_counts, atm_counts, online_counts]
channel_labels = ['Branch', 'ATM', 'Online']
plt.pie(channel_values, labels=channel_labels, autopct='%2.2f%%', colors=[palette[3], palette[4], palette[5]])
plt.title('Transactions by Channel', fontsize=12)

# 5. Customer Age Distribution (Histogram)
plt.subplot(2,3,5)
sns.histplot(data=df, x='CustomerAge', kde=True, color=palette[4])
plt.title('Customer Age Distribution', fontsize=12)
plt.xlabel('Customer Age')
plt.ylabel('Count')

# 6. Customer Occupation Distribution (Pie Chart)
student_counts = df[df['CustomerOccupation']=='Student']['CustomerOccupation'].count()
doctor_counts = df[df['CustomerOccupation']=='Doctor']['CustomerOccupation'].count()
engineer_counts = df[df['CustomerOccupation']=='Engineer']['CustomerOccupation'].count()
retired_counts = df[df['CustomerOccupation']=='Retired']['CustomerOccupation'].count()

plt.subplot(2,3,6)
occupation_values = [student_counts, doctor_counts, engineer_counts, retired_counts]
occupation_labels = ['Student', 'Doctor', 'Engineer', 'Retired']
plt.pie(occupation_values, labels=occupation_labels, autopct='%2.2f%%', colors=[palette[3], palette[4], palette[5], palette[2]])
plt.title('Transactions by Customer Occupation', fontsize=12)

# Adjust layout and show plot
plt.tight_layout()
plt.show()

"""from above univariate data analysis we got several information:
1. Majority of transactions are Debit (77.39%), with only 22.61% being Credit.
2. Skewed towards lower transaction amounts.Debit transactions dominate across all ranges, with a few higher-value Credit transactions.
3. High concentration of transactions in cities like Fort Worth, Oklahoma, and Los Angeles, with a gradual decline across other locations.
4. Transactions are evenly split across Branch (34.55%), ATM (33.16%), and Online (32.29%).
5. Peaks at ages 20–30 and 40–50, with a drop-off in the 30–40 age range and older age groups.
6. Fairly even split among Students (26.15%), Doctors (25.12%), Engineers (24.88%), and Retired individuals (23.85%).
"""

# Create a figure with 1 row and 3 columns
plt.figure(figsize=(20,6))

# 1. Daily Transaction Counts
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
df['TransactionDay'] = df['TransactionDate'].dt.date
daily_counts = df.groupby('TransactionDay').size()
plt.subplot(1,3,1)
daily_counts.plot(title='Daily Transaction Counts', color=sns.color_palette('mako')[3], linewidth=2)
plt.xlabel('Date')
plt.ylabel('Transaction Count')
plt.xticks(rotation=45)
plt.title('Daily Transaction Counts', fontsize=10)

# 2. Weekly Transaction Counts
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
df['DayOfWeek'] = df['TransactionDate'].dt.day_name()
plt.subplot(1,3,2)
sns.countplot(data=df, x='DayOfWeek', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],
              palette=sns.color_palette('mako', 5))
plt.title('Transactions by Day of Week', fontsize=10)
plt.xlabel('Day Of The Week')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')

# 3. Hourly Transaction Counts
df['Hour'] = df['TransactionDate'].dt.hour
plt.subplot(1,3,3)
sns.histplot(data=df, x='Hour', kde=True, color=sns.color_palette('mako')[3])
plt.title('Transaction Frequency by Hour', fontsize=10)
plt.xlabel('Hour of the Day')
plt.ylabel('Transaction Frequency')

df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
df['Hour of Transaction'] = df['TransactionDate'].dt.hour
df['PreviousTransactionDate'] = pd.to_datetime(df['PreviousTransactionDate'])
df['Time_Gap'] = (df['TransactionDate'] - df['PreviousTransactionDate'])
# Adjust layout and show plot
plt.tight_layout()
plt.show()

"""from above univariate transaction data accross multiple time frame we got several information:

1. Daily Transaction Counts (Left Plot):
There is high variability in daily transaction counts, fluctuating between 5 and 35 transactions per day.
No strong upward or downward trend is visible over the year, suggesting relatively stable daily activity.
Transactions by Day of the Week (Middle Plot):

2. Most transactions occur on Mondays, with a sharp drop-off on other days of the week.
The least activity seems to happen over the weekend, specifically on Saturday and Sunday.
Transaction Frequency by Hour (Right Plot):

3. Transaction activity is highest around 17:00 (5 PM), indicating a peak in user activity during the late afternoon.
A secondary peak might exist later in the evening, though the frequency significantly drops before and after the main peak at 17:00.
Let me know if you need further breakdowns or insights!

### 3.1.2 *Multivariate Analysis*
"""

sns.pairplot(df)

"""
erase some column that we don't need
Non-Numeric Features: CustomerOccupation, AccountID, and Channel...
Unique Identifiers: TransactionID, AccountID, DeviceID, and MerchantID
Redundant/Irrelevant Columns: TransactionDate, PreviousTransactionDate, IP Address
"""
df.drop(columns=['TransactionID', 'AccountID', 'TransactionDate', 'TransactionType', 'Location', 'DeviceID', 'MerchantID', 'PreviousTransactionDate', 'Channel', 'IP Address', 'CustomerOccupation'])
plt.figure(figsize=(10,6))

numeric_columns = df.select_dtypes(include=np.number).columns

correlation_matrix = df[numeric_columns].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f')
plt.xticks(rotation=90, fontsize=17)
plt.yticks(fontsize=17)
plt.title('Correlation Heatmap', fontsize=20)
plt.show()

"""The correlation heatmap provides several key insights:

1. Customer Age has a moderate positive correlation (0.32) with Account Balance, suggesting older customers tend to have higher account balances.
2. most of features have weak positive correlations with each other, implying these features are likely related to the transaction process.

# **4. *Data Preparation***
"""

# Check for duplicate rows
duplicates = df.duplicated().sum()

# Check for missing values
missing_values = df.isnull().sum().sum()

# Check for NaN values
nan_values = df.isna().sum().sum()

if duplicates == 0 and missing_values == 0 and nan_values == 0:
    print("The data is clean.")
else:
    print("The data has issues:")
    if duplicates > 0:
        print(f"There are {duplicates} duplicate rows.")
    if missing_values > 0:
        print(f"There are {missing_values} missing values.")
    if nan_values > 0:
        print(f"There are {nan_values} NaN values.")

"""
Feature Selection, this features selected because these are intuitively relevant.
"""
features = ['TransactionAmount', 'CustomerAge']
X = df[features]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# **5. *Model Development***

## **5.1 *K-means Clustering***
"""

n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)
kmeans_labels = kmeans.fit_predict(X_scaled)

df['KMeans_Cluster'] = kmeans_labels

distances = np.linalg.norm(X_scaled - kmeans.cluster_centers_[kmeans_labels], axis=1)

# Define threshold for potential frauds (top 5% farthest points from centroids)
percentile = 95
threshold = np.percentile(distances, percentile)
df['KMeans_Potential_Fraud'] = distances > threshold

frauds = df[df['KMeans_Potential_Fraud']]
non_frauds = df[~df['KMeans_Potential_Fraud']]

plt.figure(figsize=(12, 7))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=kmeans_labels, palette='viridis', s=60, alpha=0.6)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='red', label='Centroids', edgecolors='black')
plt.scatter(X_scaled[distances > threshold, 0], X_scaled[distances > threshold, 1],
            color='black', s=80, label='Potential Frauds', marker='X')
plt.title('Optimized K-means Clustering with Potential Frauds Highlighted', fontsize=14)
plt.xlabel('Scaled Transaction Amount', fontsize=12)
plt.ylabel('Scaled Customer Age', fontsize=12)
plt.legend()
plt.grid(alpha=0.3)
plt.show()

kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)
print(f"KMeans Silhouette Score: {kmeans_silhouette}")
print(f"Number of clusters: {n_clusters}")
print(f"Fraud detection threshold (distance): {threshold:.2f}")
print(f"Number of potential frauds detected: {len(frauds)}")
print(f"Fraudulent transactions account for {len(frauds) / len(df) * 100:.2f}% of all transactions.")

"""## **5.2 *DBSCAN Clustering***"""

dbscan = DBSCAN(eps=0.3, min_samples=8)  # i adjust this manually to get best Silhouette Score
dbscan_labels = dbscan.fit_predict(X_scaled)

df['DBSCAN_Potential_Fraud'] = dbscan_labels
label_mapping = {-1: 'Potential Fraud', 0: 'Normal', 1: 'Suspicious Group 1', 2: 'Suspicious Group 2'}
df['DBSCAN_Potential_Fraud'] = df['DBSCAN_Potential_Fraud'].map(label_mapping)

plt.figure(figsize=(10, 6))
for cluster in df['DBSCAN_Potential_Fraud'].unique():
    if cluster == 'Potential Fraud':
        plt.scatter(X_scaled[df['DBSCAN_Potential_Fraud'] == cluster, 0],
                    X_scaled[df['DBSCAN_Potential_Fraud'] == cluster, 1],
                    color='black', marker='x', s=60, alpha=0.6, label=cluster)
    else:
        plt.scatter(X_scaled[df['DBSCAN_Potential_Fraud'] == cluster, 0],
                    X_scaled[df['DBSCAN_Potential_Fraud'] == cluster, 1],
                    s=60, alpha=0.6, label=cluster)

plt.title('DBSCAN Clustering on Transactions')
plt.xlabel('Scaled Transaction Amount')
plt.ylabel('Scaled Customer Age')
plt.legend(title='Cluster')
plt.show()

silhouette = silhouette_score(X_scaled, dbscan_labels)
print(f"Silhouette Score: {silhouette}")

num_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"Number of clusters: {num_clusters}")

potential_frauds = df[df['DBSCAN_Potential_Fraud'] == 'Potential Fraud']
num_frauds = len(potential_frauds)
print(f"Number of potential frauds detected: {num_frauds}")

fraud_percentage = (num_frauds / len(df)) * 100
print(f"Fraudulent transactions account for {fraud_percentage:.2f}% of all transactions.")

"""## **5.3 *Isolation Forest Clustering***"""

iso_forest = IsolationForest(contamination=0.01, random_state=42)
outlier_pred = iso_forest.fit_predict(X_scaled)

outlier_mapping = {1: 'Normal', -1: 'Potential Fraud'}
df['IsoFores_Potential_Fraud'] = pd.Series(outlier_pred).map(outlier_mapping)

plt.figure(figsize=(10, 6))
for prediction in df['IsoFores_Potential_Fraud'].unique():
    if prediction == 'Potential Fraud':
        plt.scatter(X_scaled[df['IsoFores_Potential_Fraud'] == prediction, 0],
                    X_scaled[df['IsoFores_Potential_Fraud'] == prediction, 1],
                    color='black', marker='x', s=60, alpha=0.6, label=prediction)
    else:
        plt.scatter(X_scaled[df['IsoFores_Potential_Fraud'] == prediction, 0],
                    X_scaled[df['IsoFores_Potential_Fraud'] == prediction, 1],
                    s=60, alpha=0.6, label=prediction)

plt.title('Isolation Forest Outlier Detection')
plt.xlabel('Scaled Amount')
plt.ylabel('Scaled Age')
plt.legend(title='Outlier Prediction')
plt.show()

num_frauds = len(df[df['IsoFores_Potential_Fraud'] == 'Potential Fraud'])
print(f"Number of potential frauds detected: {num_frauds}")
fraud_percentage = (num_frauds / len(df)) * 100
print(f"Fraudulent transactions account for {fraud_percentage:.2f}% of all transactions.")

"""# **6. *Model Evaluation***
since this approach used unsupervised learning we will evaluate clustering model from it's Silhouette Score, result, and show threat level charts


"""

kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)
print(f"KMeans Silhouette Score: {kmeans_silhouette}")
print(f"Number of clusters: {n_clusters}")
print(f"Fraud detection threshold (distance): {threshold:.2f}")
print(f"Number of potential frauds detected: {len(frauds)}")
print(f"Fraudulent transactions account for {len(frauds) / len(df) * 100:.2f}% of all transactions.")
print("-" * 80)

DBSCAN_silhouette = silhouette_score(X_scaled, dbscan_labels)
print(f"Silhouette Score: {DBSCAN_silhouette}")
num_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"Number of clusters: {num_clusters}")
potential_frauds = df[df['DBSCAN_Potential_Fraud'] == 'Potential Fraud']
num_frauds = len(potential_frauds)
print(f"Number of potential frauds detected: {num_frauds}")
fraud_percentage = (num_frauds / len(df)) * 100
print(f"Fraudulent transactions account for {fraud_percentage:.2f}% of all transactions.")
print("-" * 80)

num_frauds = len(df[df['IsoFores_Potential_Fraud'] == 'Potential Fraud'])
print(f"Number of potential frauds detected: {num_frauds}")
fraud_percentage = (num_frauds / len(df)) * 100
print(f"Fraudulent transactions account for {fraud_percentage:.2f}% of all transactions.")

fraud_counts = {
    "KMeans": len(df[df['KMeans_Potential_Fraud'] == True]),
    "DBSCAN": len(df[df['DBSCAN_Potential_Fraud'] == 'Potential Fraud']),
    "Isolation Forest": len(df[df['IsoFores_Potential_Fraud'] == 'Potential Fraud'])
}

plt.figure(figsize=(8, 6))
plt.bar(fraud_counts.keys(), fraud_counts.values(), color=[palette[3], palette[4], palette[5]])
plt.title("Number of Fraudulent Transactions Detected by Each Model", fontsize=16)
plt.xlabel("Model", fontsize=12)
plt.ylabel("Number of Fraudulent Transactions", fontsize=12)
plt.tight_layout()
plt.show()

kmeans_true_rate = sum((df['KMeans_Potential_Fraud'] == True) &
                       (df['DBSCAN_Potential_Fraud'] == True) &
                       (df['IsoFores_Potential_Fraud'] == True)) / sum(df['KMeans_Potential_Fraud'] == True) * 100

dbscan_true_rate = sum((df['DBSCAN_Potential_Fraud'] == True) &
                       (df['KMeans_Potential_Fraud'] == True) &
                       (df['IsoFores_Potential_Fraud'] == True)) / sum(df['DBSCAN_Potential_Fraud'] == True) * 100

isoforest_true_rate = sum((df['IsoFores_Potential_Fraud'] == True) &
                          (df['KMeans_Potential_Fraud'] == True) &
                          (df['DBSCAN_Potential_Fraud'] == True)) / sum(df['IsoFores_Potential_Fraud'] == True) * 100

true_pred_rates = pd.DataFrame({
    'KMeans True Preds Rate': [kmeans_true_rate],
    'DBSCAN True Preds Rate': [dbscan_true_rate],
    'IsoForest True Preds Rate': [isoforest_true_rate]
})

plt.figure(figsize=(8, 6))
plt.bar(true_pred_rates.columns, true_pred_rates.iloc[0], color=[palette[3], palette[4], palette[5]])
plt.title("True Predictions Alignment Across Models", fontsize=16)
plt.ylabel("Percentage of True Predictions", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

df[df['KMeans_Potential_Fraud'] == True].head(10)

"""
update [IsoFores_Potential_Fraud] and [DBSCAN_Potential_Fraud] columns based on the condition where their value is "Potential Fraud",
"""
df['IsoFores_Potential_Fraud'] = df['IsoForest_Prediction'] == 'Potential Fraud'
df['DBSCAN_Potential_Fraud'] = df['DBSCAN_Cluster'] == 'Potential Fraud'

df[df['KMeans_Potential_Fraud'] == True].head(10)

top_20_data = df[df['Threat_Level'] > 1].nlargest(20, 'TransactionAmount')

heatmap_data = top_20_data.pivot_table(
    index='TransactionID',
    values='Threat_Level'
)

# Create a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(
    heatmap_data,
    annot=True,
    cmap="YlOrBr",
    vmin=0,
    vmax=3,
    cbar=True
)

plt.title("Top 20 Potential Fraud Transactions Heatmap (Threat Chart)", fontsize=16)
plt.xlabel("Threat Level", fontsize=12)
plt.ylabel("Transaction ID", fontsize=12)
plt.tight_layout()

plt.show()

"""From the evaluation carried out it was found that:

1. Kmeans clustering is the most sensitive model in predicting potential fraud, followed by DBScan, and Isolation Forest
2. In the other hand, the suitability of Isolation Forest predictions has the highest level of suitability when compared to other model answers.
3. Prediction results are better if assessed via the Threat map which is the combined result of predictions from all models

"""